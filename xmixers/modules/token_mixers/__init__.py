from .linear_attention import (
    TTT,
    ChunkRnn,
    DecayLinearAttention,
    DeltaUnit,
    DenseRnn,
    GatedSlotAttention,
    Hgru1,
    Hgru2,
    Hgru2ScalarDecay,
    Hgru3,
    LightNetAttention,
    LinearAttention,
    MetaLa,
    PolarRnn,
    TnlAttention,
)
from .long_conv import Gtu
from .vanilla_attention import (
    Attention,
    FlexAttention,
    ForgettingAttention,
    FsqKvAttention,
    FsqKvMultiProductAttention,
    MultiFactorAttention,
    MultiLatentAttention,
    MultiProductAttention,
    NaiveSparseAttention,
    SimpleSparseAttention,
    StickBreakingAttention,
    TensorProductAttention,
    nAttention,
)

AUTO_TOKEN_MIXER_MAPPING = {
    # softmax attn
    "attn": Attention,
    "flex_attn": FlexAttention,
    "n_attn": nAttention,
    "mpa": MultiProductAttention,
    "tpa": TensorProductAttention,
    "mla": MultiLatentAttention,
    "ssa": SimpleSparseAttention,
    "nsa": NaiveSparseAttention,
    "fsq_kv_attn": FsqKvAttention,
    "fsq_kv_mpa": FsqKvMultiProductAttention,
    "forgetting_attn": ForgettingAttention,
    "sb_attn": StickBreakingAttention,
    "mfa": MultiFactorAttention,
    # linear attn
    "hgru1": Hgru1,
    "hgru2": Hgru2,
    "hgru2_scalar_decay": Hgru2ScalarDecay,
    "hgru3": Hgru3,
    "linear_attn": LinearAttention,
    "tnl_attn": TnlAttention,
    "metala": MetaLa,
    "polar_rnn": PolarRnn,
    "dense_rnn": DenseRnn,
    "chunk_rnn": ChunkRnn,
    "delta_unit": DeltaUnit,
    "lightnet": LightNetAttention,
    "decay_linear_attn": DecayLinearAttention,
    "gsa": GatedSlotAttention,
    "ttt": TTT,
    # long conv
    "gtu": Gtu,
}

SOFTMAX_TOKEN_MIXER_LIST = ["attn", "flex_attn", "n_attn", "mpa", "cpa", "mla", "ssa"]
LINEAR_TOKEN_MIXER_LIST = [
    "lightnet",
    "hgru2",
    "hgru3",
    "linear_attn",
    "tnl_attn",
    "metala",
    "polar_rnn",
    "dense_rnn",
]


def get_token_mixer(config, layer_idx):
    cls = AUTO_TOKEN_MIXER_MAPPING[config.token_mixer_type]
    if config.token_mixer_type in [
        "attn",
        "flex_attn",
        "n_attn",
        "forgetting_attn",
        "sb_attn",
    ]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            kv_heads=config.kv_heads,
            bias=config.bias,
            layer_idx=layer_idx,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            window_size=config.window_size,
            init_std=config.init_std,
            gain=config.gain,
        )
    elif config.token_mixer_type in ["nsa"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            kv_heads=config.kv_heads,
            bias=config.bias,
            use_lrpe=config.use_lrpe,
            layer_idx=layer_idx,
            lrpe_type=config.lrpe_type,
            base=config.base,
            max_position_embeddings=config.max_position_embeddings,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            window_size=config.window_size,
            head_dim=config.head_dim,
            block_size=config.block_size,
            init_std=config.init_std,
            gain=config.gain,
        )
    elif config.token_mixer_type in [
        "mpa",
        "fsq_kv_mpa",
    ]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            kv_heads=config.kv_heads,
            bias=config.bias,
            use_lrpe=config.use_lrpe,
            layer_idx=layer_idx,
            lrpe_type=config.lrpe_type,
            base=config.base,
            max_position_embeddings=config.max_position_embeddings,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            window_size=config.window_size,
            mpa_type=config.mpa_type,
            mpa_activation=config.mpa_activation,
            q_rank=config.q_rank,
            center=config.center,
            head_dim=config.head_dim,
            init_std=config.init_std,
            gain=config.gain,
        )
    elif config.token_mixer_type in ["tpa"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            kv_heads=config.kv_heads,
            bias=config.bias,
            use_lrpe=config.use_lrpe,
            layer_idx=layer_idx,
            lrpe_type=config.lrpe_type,
            base=config.base,
            max_position_embeddings=config.max_position_embeddings,
            q_rank=config.q_rank,
            kv_rank=config.kv_rank,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            cp_activation=config.cp_activation,
            head_dim=config.head_dim,
            init_std=config.init_std,
            gain=config.gain,
        )
    elif config.token_mixer_type in ["mla"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            q_lora_rank=config.q_lora_rank,
            kv_lora_rank=config.kv_lora_rank,
            qk_rope_head_dim=config.qk_rope_head_dim,
            bias=config.bias,
            use_lrpe=config.use_lrpe,
            layer_idx=layer_idx,
            lrpe_type=config.lrpe_type,
            base=config.base,
            max_position_embeddings=config.max_position_embeddings,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            window_size=config.window_size,
            init_std=config.init_std,
            gain=config.gain,
        )
    elif config.token_mixer_type in ["ssa"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            kv_heads=config.kv_heads,
            bias=config.bias,
            use_lrpe=config.use_lrpe,
            layer_idx=layer_idx,
            lrpe_type=config.lrpe_type,
            base=config.base,
            max_position_embeddings=config.max_position_embeddings,
            chunk_size=config.chunk_size,
            token_mixer_top_k=config.token_mixer_top_k,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            window_size=config.window_size,
            init_std=config.init_std,
            gain=config.gain,
        )
    elif config.token_mixer_type in ["hgru2"]:
        return cls(
            embed_dim=config.embed_dim,
            expand_ratio=config.expand_ratio,
            bias=config.bias,
            layer_idx=layer_idx,
            use_output_gate=config.use_output_gate,
            norm_type=config.norm_type,
            q_activation=config.q_activation,
            causal=config.causal,
            gate_act=config.gate_act,
            gate_pos=config.gate_pos,
            rescale_type=config.rescale_type,
            token_mixer_init_type=config.token_mixer_init_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
            beta_activation=config.beta_activation,
            use_dense_memory=config.use_dense_memory,
            token_mixer_norm_type=config.token_mixer_norm_type
            if hasattr(config, "token_mixer_norm_type")
            else config.norm_type,
        )
    elif config.token_mixer_type in ["hgru2_scalar_decay"]:
        return cls(
            embed_dim=config.embed_dim,
            expand_ratio=config.expand_ratio,
            bias=config.bias,
            layer_idx=layer_idx,
            use_output_gate=config.use_output_gate,
            norm_type=config.norm_type,
            q_activation=config.q_activation,
            k_activation=config.k_activation,
            causal=config.causal,
            rescale_type=config.rescale_type,
            token_mixer_init_type=config.token_mixer_init_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
            beta_activation=config.beta_activation,
            use_dense_memory=config.use_dense_memory,
            token_mixer_norm_type=config.token_mixer_norm_type
            if hasattr(config, "token_mixer_norm_type")
            else config.norm_type,
        )
    elif config.token_mixer_type in ["hgru3"]:
        return cls(
            embed_dim=config.embed_dim,
            expand_ratio=config.expand_ratio,
            bias=config.bias,
            layer_idx=layer_idx,
            use_output_gate=config.use_output_gate,
            norm_type=config.norm_type,
            q_activation=config.q_activation,
            k_activation=config.k_activation,
            threshold=config.threshold,
            causal=config.causal,
            use_dense_memory=config.use_dense_memory,
            scalar_decay=config.scalar_decay,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
            gate_act=config.gate_act,
            gate_pos=config.gate_pos,
        )
    elif config.token_mixer_type in ["linear_attn"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            kv_heads=config.kv_heads,
            bias=config.bias,
            use_lrpe=config.use_lrpe,
            layer_idx=layer_idx,
            lrpe_type=config.lrpe_type,
            base=config.base,
            use_output_gate=config.use_output_gate,
            token_mixer_norm_type=config.token_mixer_norm_type,
            norm_type=config.norm_type,
            linear_activation=config.linear_activation,
            causal=config.causal,
            gate_act=config.gate_act,
            gate_pos=config.gate_pos,
            use_dense_memory=config.use_dense_memory,
            max_position_embeddings=config.max_position_embeddings,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
        )
    elif config.token_mixer_type in ["tnl_attn"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            bias=config.bias,
            layer_idx=layer_idx,
            use_output_gate=config.use_output_gate,
            norm_type=config.norm_type,
            q_activation=config.q_activation,
            k_activation=config.k_activation,
            v_activation=config.v_activation,
            q_norm=config.q_norm,
            k_norm=config.k_norm,
            v_norm=config.v_norm,
            causal=config.causal,
            gate_act=config.gate_act,
            gate_pos=config.gate_pos,
            token_mixer_norm_type=config.token_mixer_norm_type
            if hasattr(config, "token_mixer_norm_type")
            else config.norm_type,
            use_initial_state=config.use_initial_state,
            max_position_embeddings=config.max_position_embeddings,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
        )
    elif config.token_mixer_type in ["metala"]:
        return cls(
            embed_dim=config.embed_dim,
            expand_ratio=config.expand_ratio,
            bias=config.bias,
            layer_idx=layer_idx,
            use_output_gate=config.use_output_gate,
            non_sparse_ratio=config.non_sparse_ratio,
            num_sparse=config.num_sparse,
            norm_type=config.norm_type,
            q_activation=config.q_activation,
            causal=config.causal,
            rescale_type=config.rescale_type,
            token_mixer_init_type=config.token_mixer_init_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
        )
    elif config.token_mixer_type in ["polar_rnn"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            bias=config.bias,
            layer_idx=layer_idx,
            use_output_gate=config.use_output_gate,
            norm_type=config.norm_type,
            q_activation=config.q_activation,
            k_activation=config.k_activation,
            v_activation=config.v_activation,
            use_gamma=config.use_gamma,
            gamma_activation=config.gamma_activation,
            use_decay=config.use_decay,
            scalar_decay=config.scalar_decay,
            qkv_norm_type=config.qkv_norm_type,
            norm_q=config.norm_q,
            norm_v=config.norm_v,
            causal=config.causal,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
            debug=config.debug,
            use_l2_norm=config.use_l2_norm,
        )
    elif config.token_mixer_type in ["dense_rnn"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            bias=config.bias,
            layer_idx=layer_idx,
            use_output_gate=config.use_output_gate,
            token_mixer_norm_type=config.token_mixer_norm_type,
            q_activation=config.q_activation,
            k_activation=config.k_activation,
            v_activation=config.v_activation,
            use_beta=config.use_beta,
            beta_activation=config.beta_activation,
            qkv_norm_type=config.qkv_norm_type,
            norm_q=config.norm_q,
            norm_v=config.norm_v,
            causal=config.causal,
            max_position_embeddings=config.max_position_embeddings,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
            gate_act=config.gate_act,
            gate_pos=config.gate_pos,
        )
    elif config.token_mixer_type in ["delta_unit"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            bias=config.bias,
            layer_idx=layer_idx,
            use_output_gate=config.use_output_gate,
            token_mixer_norm_type=config.token_mixer_norm_type,
            q_activation=config.q_activation,
            k_activation=config.k_activation,
            v_activation=config.v_activation,
            use_beta=config.use_beta,
            beta_activation=config.beta_activation,
            use_decay=config.use_decay,
            scalar_decay=config.scalar_decay,
            qkv_norm_type=config.qkv_norm_type,
            norm_q=config.norm_q,
            norm_v=config.norm_v,
            causal=config.causal,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
            gate_act=config.gate_act,
            gate_pos=config.gate_pos,
        )
    elif config.token_mixer_type in ["chunk_rnn"]:
        return cls(
            embed_dim=config.embed_dim,
            expand_ratio=config.expand_ratio,
            bias=config.bias,
            layer_idx=layer_idx,
            use_output_gate=config.use_output_gate,
            norm_type=config.norm_type,
            q_activation=config.q_activation,
            causal=config.causal,
            rescale_type=config.rescale_type,
            token_mixer_init_type=config.token_mixer_init_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
            token_mixer_norm_type=config.token_mixer_norm_type
            if hasattr(config, "token_mixer_norm_type")
            else config.norm_type,
            chunk_type=config.chunk_type,
            gradient_type=config.gradient_type,
            use_init_weights=config.use_init_weights,
            use_scale=config.use_scale,
            chunk_size=config.chunk_size,
            use_lrpe=config.use_lrpe,
            lrpe_type=config.lrpe_type,
            base=config.base,
        )
    elif config.token_mixer_type in ["lightnet"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            bias=config.bias,
            layer_idx=layer_idx,
            use_lrpe=config.use_lrpe,
            lrpe_type=config.lrpe_type,
            base=config.base,
            use_output_gate=config.use_output_gate,
            token_mixer_norm_type=config.token_mixer_norm_type,
            q_activation=config.q_activation,
            k_activation=config.k_activation,
            scalar_decay=config.scalar_decay,
            causal=config.causal,
            gate_act=config.gate_act,
            gate_pos=config.gate_pos,
            use_input_gate=config.use_input_gate,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
        )
    elif config.token_mixer_type in ["decay_linear_attn"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            bias=config.bias,
            layer_idx=layer_idx,
            use_lrpe=config.use_lrpe,
            base=config.base,
            use_output_gate=config.use_output_gate,
            token_mixer_norm_type=config.token_mixer_norm_type,
            q_activation=config.q_activation,
            k_activation=config.k_activation,
            decay_type=config.decay_type,
            # decay parameters
            A_init_range=config.A_init_range,
            dt_min=config.dt_min,
            dt_max=config.dt_max,
            dt_init_floor=config.dt_init_floor,
            dt_limit=config.dt_limit,
            gate_denom=config.gate_denom,
            threshold=config.threshold,
            share_decay=config.share_decay,
            scalar_decay=config.scalar_decay,
            causal=config.causal,
            gate_act=config.gate_act,
            gate_pos=config.gate_pos,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
        )
    elif config.token_mixer_type in ["gsa"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            bias=config.bias,
            layer_idx=layer_idx,
            use_output_gate=config.use_output_gate,
            token_mixer_norm_type=config.token_mixer_norm_type,
            q_activation=config.q_activation,
            k_activation=config.k_activation,
            num_slots=config.num_slots,
            causal=config.causal,
            gate_act=config.gate_act,
            gate_pos=config.gate_pos,
            max_position_embeddings=config.max_position_embeddings,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
        )
    elif config.token_mixer_type in ["ttt"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            bias=config.bias,
            layer_idx=layer_idx,
            use_output_gate=config.use_output_gate,
            token_mixer_norm_type=config.token_mixer_norm_type,
            q_activation=config.q_activation,
            k_activation=config.k_activation,
            norm_k=config.norm_k,
            beta_activation=config.beta_activation,
            causal=config.causal,
            gate_act=config.gate_act,
            gate_pos=config.gate_pos,
            max_position_embeddings=config.max_position_embeddings,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
            use_initial_state=config.use_initial_state,
        )
    elif config.token_mixer_type in ["fsq_kv_attn"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            kv_heads=config.kv_heads,
            bias=config.bias,
            use_lrpe=config.use_lrpe,
            layer_idx=layer_idx,
            lrpe_type=config.lrpe_type,
            base=config.base,
            max_position_embeddings=config.max_position_embeddings,
            token_mixer_init_type=config.token_mixer_init_type,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            window_size=config.window_size,
            init_std=config.init_std,
            gain=config.gain,
            num_bins=config.num_bins,
            center=config.center,
            use_proj=config.use_proj,
            share_proj=config.share_proj,
        )
    elif config.token_mixer_type in ["mfa"]:
        return cls(
            embed_dim=config.embed_dim,
            num_heads=config.num_heads,
            kv_heads=config.kv_heads,
            bias=config.bias,
            use_lrpe=config.use_lrpe,
            layer_idx=layer_idx,
            lrpe_type=config.lrpe_type,
            base=config.base,
            max_position_embeddings=config.max_position_embeddings,
            token_mixer_init_type=config.token_mixer_init_type,
            share_kv=config.share_kv,
            head_dim=config.head_dim,
            rescale_type=config.rescale_type,
            num_layers=config.num_layers,
            window_size=config.window_size,
            init_std=config.init_std,
            gain=config.gain,
        )
    elif config.token_mixer_type in ["hgru1"]:
        return cls(
            embed_dim=config.embed_dim,
            head_dim=config.head_dim,
            bias=config.bias,
            layer_idx=layer_idx,
            use_output_gate=config.use_output_gate,
            norm_type=config.norm_type,
            q_activation=config.q_activation,
            causal=config.causal,
            gate_act=config.gate_act,
            gate_pos=config.gate_pos,
            rescale_type=config.rescale_type,
            token_mixer_init_type=config.token_mixer_init_type,
            num_layers=config.num_layers,
            init_std=config.init_std,
            gain=config.gain,
            beta_activation=config.beta_activation,
            use_dense_memory=config.use_dense_memory,
            token_mixer_norm_type=config.token_mixer_norm_type
            if hasattr(config, "token_mixer_norm_type")
            else config.norm_type,
        )
